{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting music21\n",
      "  Downloading music21-7.1.0.tar.gz (19.2 MB)\n",
      "Requirement already satisfied: chardet in c:\\users\\sam\\anaconda3_new\\lib\\site-packages (from music21) (3.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\sam\\anaconda3_new\\lib\\site-packages (from music21) (0.17.0)\n",
      "Collecting jsonpickle\n",
      "  Downloading jsonpickle-2.0.0-py2.py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\sam\\anaconda3_new\\lib\\site-packages (from music21) (3.3.2)\n",
      "Requirement already satisfied: more_itertools in c:\\users\\sam\\anaconda3_new\\lib\\site-packages (from music21) (8.6.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\sam\\anaconda3_new\\lib\\site-packages (from music21) (1.20.3)\n",
      "Collecting webcolors>=1.5\n",
      "  Downloading webcolors-1.11.1-py3-none-any.whl (9.9 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\sam\\anaconda3_new\\lib\\site-packages (from matplotlib->music21) (2.8.1)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in c:\\users\\sam\\anaconda3_new\\lib\\site-packages (from matplotlib->music21) (2020.6.20)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\sam\\anaconda3_new\\lib\\site-packages (from matplotlib->music21) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\sam\\anaconda3_new\\lib\\site-packages (from matplotlib->music21) (1.3.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\sam\\anaconda3_new\\lib\\site-packages (from matplotlib->music21) (8.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\sam\\anaconda3_new\\lib\\site-packages (from matplotlib->music21) (0.10.0)\n",
      "Requirement already satisfied: six in c:\\users\\sam\\anaconda3_new\\lib\\site-packages (from cycler>=0.10->matplotlib->music21) (1.15.0)\n",
      "Building wheels for collected packages: music21\n",
      "  Building wheel for music21 (setup.py): started\n",
      "  Building wheel for music21 (setup.py): finished with status 'done'\n",
      "  Created wheel for music21: filename=music21-7.1.0-py3-none-any.whl size=21912606 sha256=2cafdb8a92266b0dd482c8e18c93950831885ce244ad6f69ebf2a53515479859\n",
      "  Stored in directory: c:\\users\\sam\\appdata\\local\\pip\\cache\\wheels\\c3\\0a\\1a\\73580f7fc6d1a676a949544d592c0a39546b8ba0477594f2c9\n",
      "Successfully built music21\n",
      "Installing collected packages: webcolors, jsonpickle, music21\n",
      "Successfully installed jsonpickle-2.0.0 music21-7.1.0 webcolors-1.11.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\sam\\anaconda3_new\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\sam\\anaconda3_new\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\sam\\anaconda3_new\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\sam\\anaconda3_new\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\sam\\anaconda3_new\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\sam\\anaconda3_new\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\sam\\anaconda3_new\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\sam\\anaconda3_new\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\sam\\anaconda3_new\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.1.2; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\sam\\anaconda3_new\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install music21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (<ipython-input-8-7b5efee8c6c0>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-7b5efee8c6c0>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    'C:\\Users\\sam\\anaconda3_new\\python.exe -m pip install --upgrade pip'\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "C:\\Users\\sam\\anaconda3_new\\python.exe -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for listing down the file names\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Array Processing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify the path\n",
    "path='schubert/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read all the filenames\n",
    "files=[i for i in os.listdir(path) if i.endswith(\".mid\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining function to read MIDI files\n",
    "def read_midi(file):\n",
    "    \n",
    "    print(\"Loading Music File:\",file)\n",
    "    \n",
    "    notes=[]\n",
    "    notes_to_parse = None\n",
    "    \n",
    "    #parsing a midi file\n",
    "    midi = converter.parse(file)\n",
    "  \n",
    "    #grouping based on different instruments\n",
    "    s2 = instrument.partitionByInstrument(midi)\n",
    "\n",
    "    #Looping over all the instruments\n",
    "    for part in s2.parts:\n",
    "    \n",
    "        #select elements of only piano\n",
    "        if 'Piano' in str(part): \n",
    "        \n",
    "            notes_to_parse = part.recurse() \n",
    "      \n",
    "            #finding whether a particular element is note or a chord\n",
    "            for element in notes_to_parse:\n",
    "                \n",
    "                #note\n",
    "                if isinstance(element, note.Note):\n",
    "                    notes.append(str(element.pitch))\n",
    "                \n",
    "                #chord\n",
    "                elif isinstance(element, chord.Chord):\n",
    "                    notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "    return np.array(notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Music File: schubert/schubert_D850_1.mid\n",
      "Loading Music File: schubert/schubert_D850_2.mid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sam\\anaconda3_new\\lib\\site-packages\\music21\\midi\\translate.py:785: TranslateWarning: Unable to determine instrument from <music21.midi.MidiEvent SEQUENCE_TRACK_NAME, track=5, channel=None, data=b'Copyright \\xa9 2010 by Bernd Krueger'>; getting generic Instrument\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Music File: schubert/schubert_D850_3.mid\n",
      "Loading Music File: schubert/schubert_D850_4.mid\n",
      "Loading Music File: schubert/schubert_D935_1.mid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sam\\anaconda3_new\\lib\\site-packages\\music21\\midi\\translate.py:785: TranslateWarning: Unable to determine instrument from <music21.midi.MidiEvent SEQUENCE_TRACK_NAME, track=5, channel=None, data=b'Copyright \\xa9 2007 by Bernd Krueger'>; getting generic Instrument\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Music File: schubert/schubert_D935_2.mid\n",
      "Loading Music File: schubert/schubert_D935_3.mid\n",
      "Loading Music File: schubert/schubert_D935_4.mid\n",
      "Loading Music File: schubert/schub_d760_1.mid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sam\\anaconda3_new\\lib\\site-packages\\music21\\midi\\translate.py:785: TranslateWarning: Unable to determine instrument from <music21.midi.MidiEvent SEQUENCE_TRACK_NAME, track=4, channel=None, data=b'Copyright \\xa9 2004 by Bernd Kr\\xfcger'>; getting generic Instrument\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Music File: schubert/schub_d760_2.mid\n",
      "Loading Music File: schubert/schub_d760_3.mid\n",
      "Loading Music File: schubert/schub_d760_4.mid\n",
      "Loading Music File: schubert/schub_d960_1.mid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sam\\anaconda3_new\\lib\\site-packages\\music21\\midi\\translate.py:785: TranslateWarning: Unable to determine instrument from <music21.midi.MidiEvent SEQUENCE_TRACK_NAME, track=5, channel=None, data=b'Copyright \\xa9 2002 by Bernd Kr\\xfcger'>; getting generic Instrument\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Music File: schubert/schub_d960_2.mid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sam\\anaconda3_new\\lib\\site-packages\\music21\\midi\\translate.py:785: TranslateWarning: Unable to determine instrument from <music21.midi.MidiEvent SEQUENCE_TRACK_NAME, track=4, channel=None, data=b'Copyright \\xa9 2002 by Bernd Kr\\xfcger'>; getting generic Instrument\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Music File: schubert/schub_d960_3.mid\n",
      "Loading Music File: schubert/schub_d960_4.mid\n",
      "Loading Music File: schubert/schuim-1.mid\n",
      "Loading Music File: schubert/schuim-2.mid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sam\\anaconda3_new\\lib\\site-packages\\music21\\midi\\translate.py:785: TranslateWarning: Unable to determine instrument from <music21.midi.MidiEvent SEQUENCE_TRACK_NAME, track=4, channel=None, data=b'Copyright \\xa9 1997 by Bernd Krueger'>; getting generic Instrument\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Music File: schubert/schuim-3.mid\n",
      "Loading Music File: schubert/schuim-4.mid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sam\\anaconda3_new\\lib\\site-packages\\music21\\midi\\translate.py:785: TranslateWarning: Unable to determine instrument from <music21.midi.MidiEvent SEQUENCE_TRACK_NAME, track=4, channel=None, data=b'Copyright \\xa9 1996 by Bernd Krueger'>; getting generic Instrument\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Music File: schubert/schumm-1.mid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sam\\anaconda3_new\\lib\\site-packages\\music21\\midi\\translate.py:785: TranslateWarning: Unable to determine instrument from <music21.midi.MidiEvent SEQUENCE_TRACK_NAME, track=4, channel=None, data=b'Copyright \\xa9 1999 by Bernd Krueger'>; getting generic Instrument\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Music File: schubert/schumm-2.mid\n",
      "Loading Music File: schubert/schumm-3.mid\n",
      "Loading Music File: schubert/schumm-4.mid\n",
      "Loading Music File: schubert/schumm-5.mid\n",
      "Loading Music File: schubert/schumm-6.mid\n",
      "Loading Music File: schubert/schu_143_1.mid\n",
      "Loading Music File: schubert/schu_143_2.mid\n",
      "Loading Music File: schubert/schu_143_3.mid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sam\\anaconda3_new\\lib\\site-packages\\music21\\midi\\translate.py:785: TranslateWarning: Unable to determine instrument from <music21.midi.MidiEvent SEQUENCE_TRACK_NAME, track=4, channel=None, data=b'Copyright \\xa9 1999 by Bernd Kr\\xfcger'>; getting generic Instrument\n",
      "  warnings.warn(\n",
      "<ipython-input-48-371224d13292>:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  notes_array = np.array([read_midi(path+i) for i in files])\n"
     ]
    }
   ],
   "source": [
    "#reading each midi file\n",
    "notes_array = np.array([read_midi(path+i) for i in files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Understanding the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345\n"
     ]
    }
   ],
   "source": [
    "#converting 2D array into 1D array\n",
    "notes_ = [element for note_ in notes_array for element in note_]\n",
    "\n",
    "#No. of unique notes\n",
    "unique_notes = list(set(notes_))\n",
    "print(len(unique_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As you can see here, no. of unique notes is 304. Now, let us see the distribution of the notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([221.,  40.,  31.,  13.,   8.,   3.,   7.,   9.,   7.,   6.]),\n",
       " array([1.0000e+00, 1.6170e+02, 3.2240e+02, 4.8310e+02, 6.4380e+02,\n",
       "        8.0450e+02, 9.6520e+02, 1.1259e+03, 1.2866e+03, 1.4473e+03,\n",
       "        1.6080e+03]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 302,
       "width": 319
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#importing library\n",
    "from collections import Counter\n",
    "\n",
    "#computing frequency of each note\n",
    "freq = dict(Counter(notes_))\n",
    "\n",
    "#library for visualiation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#consider only the frequencies\n",
    "no=[count for _,count in freq.items()]\n",
    "\n",
    "#set the figure size\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "#plot\n",
    "plt.hist(no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the above plot, we can infer that most of the notes have a very low frequency. So, let us keep the top frequent notes and ignore the low-frequency ones. Here, I am defining the threshold as 50. Nevertheless, the parameter can be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182\n"
     ]
    }
   ],
   "source": [
    "frequent_notes = [note_ for note_, count in freq.items() if count>=50]\n",
    "print(len(frequent_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As you can see here, no. of frequently occurring notes is around 170.  Now, let us prepare new musical files which contain only the top frequent notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-56-2fa7923ce1b3>:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  new_music = np.array(new_music)\n"
     ]
    }
   ],
   "source": [
    "new_music=[]\n",
    "\n",
    "for notes in notes_array:\n",
    "    temp=[]\n",
    "    for note_ in notes:\n",
    "        if note_ in frequent_notes:\n",
    "            temp.append(note_)            \n",
    "    new_music.append(temp)\n",
    "    \n",
    "new_music = np.array(new_music)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing the input and output sequences as mentioned in the article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_timesteps = 32\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for note_ in new_music:\n",
    "    for i in range(0, len(note_) - no_of_timesteps, 1):\n",
    "        \n",
    "        #preparing input and output sequences\n",
    "        input_ = note_[i:i + no_of_timesteps]\n",
    "        output = note_[i + no_of_timesteps]\n",
    "        \n",
    "        x.append(input_)\n",
    "        y.append(output)\n",
    "        \n",
    "x=np.array(x)\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, we will assign a unique integer to every note:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_x = list(set(x.ravel()))\n",
    "x_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will prepare the integer sequences for input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing input sequences\n",
    "x_seq=[]\n",
    "for i in x:\n",
    "    temp=[]\n",
    "    for j in i:\n",
    "        #assigning unique integer to every note\n",
    "        temp.append(x_note_to_int[j])\n",
    "    x_seq.append(temp)\n",
    "    \n",
    "x_seq = np.array(x_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Similarly, prepare the integer sequences for output data as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_y = list(set(y))\n",
    "y_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_y)) \n",
    "y_seq=np.array([y_note_to_int[i] for i in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us preserve 80% of the data for training and the rest 20% for the evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_tr, x_val, y_tr, y_val = train_test_split(x_seq,y_seq,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Building\n",
    "\n",
    "#I have defined 2 architectures here – WaveNet and LSTM. Please experiment with both the architectures to understand the importance of WaveNet architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm():\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(128,return_sequences=True))\n",
    "  model.add(LSTM(128))\n",
    "  model.add(Dense(256))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dense(n_vocab))\n",
    "  model.add(Activation('softmax'))\n",
    "  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I have simplified the architecture of the WaveNet without adding residual and skip connections since the role of these layers is to improve the faster convergence (and WaveNet takes raw audio wave as input). But in our case, the input would be a set of nodes and chords since we are generating music:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 32, 100)           18200     \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 32, 64)            19264     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 16, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16, 128)           24704     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 8, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 8, 256)            98560     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 4, 256)            0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 182)               46774     \n",
      "=================================================================\n",
      "Total params: 273,294\n",
      "Trainable params: 273,294\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.callbacks import *\n",
    "import keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "model = Sequential()\n",
    "    \n",
    "#embedding layer\n",
    "model.add(Embedding(len(unique_x), 100, input_length=32,trainable=True)) \n",
    "\n",
    "model.add(Conv1D(64,3, padding='causal',activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPool1D(2))\n",
    "    \n",
    "model.add(Conv1D(128,3,activation='relu',dilation_rate=2,padding='causal'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPool1D(2))\n",
    "\n",
    "model.add(Conv1D(256,3,activation='relu',dilation_rate=4,padding='causal'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPool1D(2))\n",
    "          \n",
    "#model.add(Conv1D(256,5,activation='relu'))    \n",
    "model.add(GlobalMaxPool1D())\n",
    "    \n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(unique_y), activation='softmax'))\n",
    "    \n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the callback to save the best model during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc=ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let’s train the model with a batch size of 128 for 50 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "492/492 [==============================] - 86s 94ms/step - loss: 4.6334 - val_loss: 4.1016\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.10159, saving model to best_model.h5\n",
      "Epoch 2/50\n",
      "492/492 [==============================] - 44s 90ms/step - loss: 3.9089 - val_loss: 3.9337\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.10159 to 3.93372, saving model to best_model.h5\n",
      "Epoch 3/50\n",
      "492/492 [==============================] - 44s 89ms/step - loss: 3.7127 - val_loss: 3.7909\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.93372 to 3.79093, saving model to best_model.h5\n",
      "Epoch 4/50\n",
      "492/492 [==============================] - 44s 90ms/step - loss: 3.5825 - val_loss: 3.7234\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.79093 to 3.72338, saving model to best_model.h5\n",
      "Epoch 5/50\n",
      "492/492 [==============================] - 44s 90ms/step - loss: 3.4993 - val_loss: 3.6353\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.72338 to 3.63532, saving model to best_model.h5\n",
      "Epoch 6/50\n",
      "492/492 [==============================] - 44s 90ms/step - loss: 3.4148 - val_loss: 3.6164\n",
      "\n",
      "Epoch 00006: val_loss improved from 3.63532 to 3.61641, saving model to best_model.h5\n",
      "Epoch 7/50\n",
      "492/492 [==============================] - 42s 86ms/step - loss: 3.3364 - val_loss: 3.5447\n",
      "\n",
      "Epoch 00007: val_loss improved from 3.61641 to 3.54471, saving model to best_model.h5\n",
      "Epoch 8/50\n",
      "492/492 [==============================] - 35s 71ms/step - loss: 3.2894 - val_loss: 3.5261\n",
      "\n",
      "Epoch 00008: val_loss improved from 3.54471 to 3.52612, saving model to best_model.h5\n",
      "Epoch 9/50\n",
      "492/492 [==============================] - 35s 70ms/step - loss: 3.2365 - val_loss: 3.4926\n",
      "\n",
      "Epoch 00009: val_loss improved from 3.52612 to 3.49255, saving model to best_model.h5\n",
      "Epoch 10/50\n",
      "492/492 [==============================] - 35s 71ms/step - loss: 3.1923 - val_loss: 3.5123\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 3.49255\n",
      "Epoch 11/50\n",
      "492/492 [==============================] - 35s 71ms/step - loss: 3.1481 - val_loss: 3.4138\n",
      "\n",
      "Epoch 00011: val_loss improved from 3.49255 to 3.41378, saving model to best_model.h5\n",
      "Epoch 12/50\n",
      "492/492 [==============================] - 35s 70ms/step - loss: 3.1022 - val_loss: 3.4008\n",
      "\n",
      "Epoch 00012: val_loss improved from 3.41378 to 3.40084, saving model to best_model.h5\n",
      "Epoch 13/50\n",
      "492/492 [==============================] - 35s 70ms/step - loss: 3.0844 - val_loss: 3.3656\n",
      "\n",
      "Epoch 00013: val_loss improved from 3.40084 to 3.36559, saving model to best_model.h5\n",
      "Epoch 14/50\n",
      "492/492 [==============================] - 35s 71ms/step - loss: 3.0388 - val_loss: 3.3492\n",
      "\n",
      "Epoch 00014: val_loss improved from 3.36559 to 3.34916, saving model to best_model.h5\n",
      "Epoch 15/50\n",
      "492/492 [==============================] - 35s 70ms/step - loss: 3.0038 - val_loss: 3.3403\n",
      "\n",
      "Epoch 00015: val_loss improved from 3.34916 to 3.34033, saving model to best_model.h5\n",
      "Epoch 16/50\n",
      "492/492 [==============================] - 35s 71ms/step - loss: 2.9858 - val_loss: 3.3255\n",
      "\n",
      "Epoch 00016: val_loss improved from 3.34033 to 3.32553, saving model to best_model.h5\n",
      "Epoch 17/50\n",
      "492/492 [==============================] - 35s 71ms/step - loss: 2.9599 - val_loss: 3.3009\n",
      "\n",
      "Epoch 00017: val_loss improved from 3.32553 to 3.30093, saving model to best_model.h5\n",
      "Epoch 18/50\n",
      "492/492 [==============================] - 35s 70ms/step - loss: 2.9405 - val_loss: 3.3003\n",
      "\n",
      "Epoch 00018: val_loss improved from 3.30093 to 3.30032, saving model to best_model.h5\n",
      "Epoch 19/50\n",
      "492/492 [==============================] - 35s 70ms/step - loss: 2.8984 - val_loss: 3.2880\n",
      "\n",
      "Epoch 00019: val_loss improved from 3.30032 to 3.28805, saving model to best_model.h5\n",
      "Epoch 20/50\n",
      "492/492 [==============================] - 35s 72ms/step - loss: 2.8816 - val_loss: 3.2788\n",
      "\n",
      "Epoch 00020: val_loss improved from 3.28805 to 3.27881, saving model to best_model.h5\n",
      "Epoch 21/50\n",
      "492/492 [==============================] - 34s 70ms/step - loss: 2.8675 - val_loss: 3.2701\n",
      "\n",
      "Epoch 00021: val_loss improved from 3.27881 to 3.27009, saving model to best_model.h5\n",
      "Epoch 22/50\n",
      "492/492 [==============================] - 35s 70ms/step - loss: 2.8450 - val_loss: 3.2447\n",
      "\n",
      "Epoch 00022: val_loss improved from 3.27009 to 3.24467, saving model to best_model.h5\n",
      "Epoch 23/50\n",
      "492/492 [==============================] - 35s 71ms/step - loss: 2.8259 - val_loss: 3.2515\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 3.24467\n",
      "Epoch 24/50\n",
      "492/492 [==============================] - 35s 70ms/step - loss: 2.8206 - val_loss: 3.2324\n",
      "\n",
      "Epoch 00024: val_loss improved from 3.24467 to 3.23240, saving model to best_model.h5\n",
      "Epoch 25/50\n",
      "492/492 [==============================] - 35s 70ms/step - loss: 2.8072 - val_loss: 3.2160\n",
      "\n",
      "Epoch 00025: val_loss improved from 3.23240 to 3.21604, saving model to best_model.h5\n",
      "Epoch 26/50\n",
      "492/492 [==============================] - 35s 70ms/step - loss: 2.7752 - val_loss: 3.2157\n",
      "\n",
      "Epoch 00026: val_loss improved from 3.21604 to 3.21568, saving model to best_model.h5\n",
      "Epoch 27/50\n",
      "492/492 [==============================] - 35s 71ms/step - loss: 2.7728 - val_loss: 3.1952\n",
      "\n",
      "Epoch 00027: val_loss improved from 3.21568 to 3.19520, saving model to best_model.h5\n",
      "Epoch 28/50\n",
      "492/492 [==============================] - 35s 71ms/step - loss: 2.7648 - val_loss: 3.2032\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 3.19520\n",
      "Epoch 29/50\n",
      "492/492 [==============================] - 35s 71ms/step - loss: 2.7554 - val_loss: 3.1947\n",
      "\n",
      "Epoch 00029: val_loss improved from 3.19520 to 3.19467, saving model to best_model.h5\n",
      "Epoch 30/50\n",
      "492/492 [==============================] - 35s 71ms/step - loss: 2.7235 - val_loss: 3.1962\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 3.19467\n",
      "Epoch 31/50\n",
      "492/492 [==============================] - 35s 71ms/step - loss: 2.7306 - val_loss: 3.1846\n",
      "\n",
      "Epoch 00031: val_loss improved from 3.19467 to 3.18463, saving model to best_model.h5\n",
      "Epoch 32/50\n",
      "492/492 [==============================] - 35s 71ms/step - loss: 2.7150 - val_loss: 3.1853\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 3.18463\n",
      "Epoch 33/50\n",
      "492/492 [==============================] - 35s 72ms/step - loss: 2.7136 - val_loss: 3.1838\n",
      "\n",
      "Epoch 00033: val_loss improved from 3.18463 to 3.18383, saving model to best_model.h5\n",
      "Epoch 34/50\n",
      "492/492 [==============================] - 35s 71ms/step - loss: 2.7014 - val_loss: 3.1747\n",
      "\n",
      "Epoch 00034: val_loss improved from 3.18383 to 3.17466, saving model to best_model.h5\n",
      "Epoch 35/50\n",
      "492/492 [==============================] - 35s 71ms/step - loss: 2.6851 - val_loss: 3.1863\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 3.17466\n",
      "Epoch 36/50\n",
      "492/492 [==============================] - 35s 71ms/step - loss: 2.6757 - val_loss: 3.1647\n",
      "\n",
      "Epoch 00036: val_loss improved from 3.17466 to 3.16465, saving model to best_model.h5\n",
      "Epoch 37/50\n",
      "492/492 [==============================] - 35s 71ms/step - loss: 2.6754 - val_loss: 3.1697\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 3.16465\n",
      "Epoch 38/50\n",
      "492/492 [==============================] - 35s 71ms/step - loss: 2.6695 - val_loss: 3.1570\n",
      "\n",
      "Epoch 00038: val_loss improved from 3.16465 to 3.15701, saving model to best_model.h5\n",
      "Epoch 39/50\n",
      "492/492 [==============================] - 35s 71ms/step - loss: 2.6588 - val_loss: 3.1592\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 3.15701\n",
      "Epoch 40/50\n",
      "492/492 [==============================] - 35s 71ms/step - loss: 2.6419 - val_loss: 3.1612\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 3.15701\n",
      "Epoch 41/50\n",
      "492/492 [==============================] - 35s 71ms/step - loss: 2.6491 - val_loss: 3.1585\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 3.15701\n",
      "Epoch 42/50\n",
      "492/492 [==============================] - 35s 71ms/step - loss: 2.6354 - val_loss: 3.1508\n",
      "\n",
      "Epoch 00042: val_loss improved from 3.15701 to 3.15076, saving model to best_model.h5\n",
      "Epoch 43/50\n",
      "492/492 [==============================] - 35s 71ms/step - loss: 2.6211 - val_loss: 3.1609\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 3.15076\n",
      "Epoch 44/50\n",
      "492/492 [==============================] - 35s 72ms/step - loss: 2.6247 - val_loss: 3.1626\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 3.15076\n",
      "Epoch 45/50\n",
      "492/492 [==============================] - 35s 71ms/step - loss: 2.6042 - val_loss: 3.1488\n",
      "\n",
      "Epoch 00045: val_loss improved from 3.15076 to 3.14877, saving model to best_model.h5\n",
      "Epoch 46/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "492/492 [==============================] - 35s 70ms/step - loss: 2.5966 - val_loss: 3.1467\n",
      "\n",
      "Epoch 00046: val_loss improved from 3.14877 to 3.14672, saving model to best_model.h5\n",
      "Epoch 47/50\n",
      "492/492 [==============================] - 35s 70ms/step - loss: 2.5937 - val_loss: 3.1438\n",
      "\n",
      "Epoch 00047: val_loss improved from 3.14672 to 3.14378, saving model to best_model.h5\n",
      "Epoch 48/50\n",
      "492/492 [==============================] - 34s 70ms/step - loss: 2.5846 - val_loss: 3.1425\n",
      "\n",
      "Epoch 00048: val_loss improved from 3.14378 to 3.14251, saving model to best_model.h5\n",
      "Epoch 49/50\n",
      "492/492 [==============================] - 35s 70ms/step - loss: 2.5829 - val_loss: 3.1323\n",
      "\n",
      "Epoch 00049: val_loss improved from 3.14251 to 3.13229, saving model to best_model.h5\n",
      "Epoch 50/50\n",
      "492/492 [==============================] - 35s 71ms/step - loss: 2.5876 - val_loss: 3.1350\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 3.13229\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(np.array(x_tr),np.array(y_tr),batch_size=128,epochs=50, validation_data=(np.array(x_val),np.array(y_val)),verbose=1, callbacks=[mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading best model\n",
    "from keras.models import load_model\n",
    "model = load_model('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Its time to compose our own music now. We will follow the steps mentioned under the inference phase for the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 92, 0, 0, 0, 92, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "ind = np.random.randint(0,len(x_val)-1)\n",
    "\n",
    "random_music = x_val[ind]\n",
    "\n",
    "predictions=[]\n",
    "for i in range(10):\n",
    "\n",
    "    random_music = random_music.reshape(1,no_of_timesteps)\n",
    "\n",
    "    prob  = model.predict(random_music)[0]\n",
    "    y_pred= np.argmax(prob,axis=0)\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "    random_music = np.insert(random_music[0],len(random_music[0]),y_pred)\n",
    "    random_music = random_music[1:]\n",
    "    \n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, we will convert the integers back into the notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_int_to_note = dict((number, note_) for number, note_ in enumerate(unique_x)) \n",
    "predicted_notes = [x_int_to_note[i] for i in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The final step is to convert back the predictions into a MIDI file. Let’s define the function to accomplish the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_midi(prediction_output):\n",
    "   \n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for pattern in prediction_output:\n",
    "        \n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                \n",
    "                cn=int(current_note)\n",
    "                new_note = note.Note(cn)\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "                \n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "            \n",
    "        # pattern is a note\n",
    "        else:\n",
    "            \n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += 1\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    midi_stream.write('midi', fp='music.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the predictions into a musical file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_midi(predicted_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some of the tunes composed by the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Awesome, right? But your learning doesn’t stop here. Just remember that we have built a baseline model. There are plenty of ways to improve the performance of the model even further:\n",
    "#As the size of the training dataset is small, we can fine-tune a pre-trained model to build a robust system\n",
    "#Collect as much as training data as you can since the deep learning model generalizes well on the larger datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def playMidi(file):\n",
    "    mf=midi.MidiFile()\n",
    "    mf.open(file)\n",
    "    mf.read()\n",
    "    mf.close()\n",
    "    s=midi.translate.midiFileToStream(mf)\n",
    "    s.show('midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <div id='midiPlayerDiv11484062'></div>\n",
       "                <link rel=\"stylesheet\" href=\"//cuthbertLab.github.io/music21j/css/m21.css\"\n",
       "                    type=\"text/css\" />\n",
       "                <script>\n",
       "                require.config({\n",
       "                    paths: {'music21': '//cuthbertLab.github.io/music21j/src/music21'}\n",
       "                });\n",
       "                require(['music21'], function() {\n",
       "                               mp = new music21.miditools.MidiPlayer();\n",
       "                               mp.addPlayer('#midiPlayerDiv11484062');\n",
       "                               mp.base64Load('data:audio/midi;base64,TVRoZAAAAAYAAQACBABNVHJrAAAAFAD/UQMHoSAA/1gEBAIYCIgA/y8ATVRyawAAAGsA/wMAAOAAQADAAIgAkDlaiACAOQAAkDJaiACAMgAAkDlaiACAOQAAkDlaiACAOQAAkDlaiACAOQAAkDJaiACAMgAAkDlaiACAOQAAkDlaiACAOQAAkDlaiACAOQAAkDlaiACAOQCIAP8vAA==');\n",
       "                        });\n",
       "                </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "playMidi('music.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
